{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "Note: sometimes your answer doesn't match one of the options exactly. That's fine. Select the option that's closest to your solution.\n",
    "\n",
    "In this homework, we will use the Car price dataset like last week. Download it from here.\n",
    "\n",
    "Or you can do it with wget:\n",
    "\n",
    "wget https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv\n",
    "We'll work with the MSRP variable, and we'll transform it to a classification task.\n",
    "\n",
    "For the rest of the homework, you'll need to use only these columns:\n",
    "\n",
    "Make,\n",
    "Model,\n",
    "Year,\n",
    "Engine HP,\n",
    "Engine Cylinders,\n",
    "Transmission Type,\n",
    "Vehicle Style,\n",
    "highway MPG,\n",
    "city mpg\n",
    "MSRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Make', 'Model', 'Year', 'Engine HP', 'Engine Cylinders', 'Transmission Type', 'Vehicle Style', 'highway MPG', 'city mpg', 'MSRP']\n",
    "\n",
    "df = df_full[columns_to_keep]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Keep only the columns above\n",
    "Lowercase the column names and replace spaces with underscores\n",
    "Fill the missing values with 0\n",
    "Make the price binary (1 if above the average, 0 otherwise) - this will be our target variable above_average\n",
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use train_test_split function for that with random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'msrp':'price'}, inplace = True) \n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns/features have NAs/Nulls?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NAs by zero\n",
    "df.fillna(0, inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: ROC AUC feature importance\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables.\n",
    "\n",
    "Let's do that\n",
    "\n",
    "For each numerical variable, use it as score and compute AUC with the above_average variable\n",
    "Use the training dataset for that\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. -df_train['engine_hp'])\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "engine_hp\n",
    "engine_cylinders\n",
    "highway_mpg\n",
    "city_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a variable above_average which is 1 if the price is above its mean value and 0 otherwise.\n",
    "df['above_average'] = (df.price > df.price.mean()).astype(int)\n",
    "#df['above_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training = df.copy()\n",
    "del df_for_training['price']\n",
    "categorical = ['make', 'model', 'transmission_type', 'vehicle_style']\n",
    "numerical = ['year', 'engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'price']\n",
    "\n",
    "df_train_full, df_test = train_test_split(df_for_training, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)\n",
    "y_train = df_train.above_average.values\n",
    "y_val = df_val.above_average.values\n",
    "y_test = df_test.above_average.values\n",
    "\n",
    "del df_train['above_average']\n",
    "del df_val['above_average']\n",
    "del df_test['above_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = numerical[:-1] # Drop 'price' from features \n",
    "\n",
    "# CALCULATING ROC AUC SCORE PER FEATURE:\n",
    "for c in numerical:\n",
    "    c_above_average = (df_train[c]>=df_train[c].mean()).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_train, c_above_average)\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(y_train, -c_above_average)\n",
    "        \n",
    "    print('%s, %.3f' % (c, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> engine_hp has the highest AUC </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional analysis to verify that engine_hp has the highest AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model_one_feature = LogisticRegression(solver='liblinear', C=1, max_iter=1000, random_state=1)\n",
    "model_one_feature.fit(np.array(df_train['engine_hp']).reshape(-1, 1),y_train)\n",
    "y_pred = model_one_feature.predict(np.array(df_val['engine_hp']).reshape(-1, 1))\n",
    "engine_hp_accuracy = round(accuracy_score(y_val, y_pred),3)\n",
    "print(f\"Accuracy on the validation dataset with just engine_hp: {engine_hp_accuracy}\")\n",
    "\n",
    "engine_hp_auc = round(roc_auc_score(y_val, y_pred),3)\n",
    "print(f\"AUC on the validation dataset with just engine_hp: {engine_hp_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_feature = LogisticRegression(solver='liblinear', C=1, max_iter=1000, random_state=1)\n",
    "model_one_feature.fit(np.array(df_train['engine_cylinders']).reshape(-1, 1),y_train)\n",
    "y_pred = model_one_feature.predict(np.array(df_val['engine_cylinders']).reshape(-1, 1))\n",
    "engine_cylinders_accuracy = round(accuracy_score(y_val, y_pred),3)\n",
    "print(f\"Accuracy on the validation dataset with engine_cylinders: {engine_cylinders_accuracy}\")\n",
    "\n",
    "engine_cylinders_auc = round(roc_auc_score(y_val, y_pred),3)\n",
    "print(f\"AUC on the validation dataset with just engine_cylinders: {engine_cylinders_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_feature = LogisticRegression(solver='liblinear', C=1, max_iter=1000, random_state=1)\n",
    "model_one_feature.fit(np.array(df_train['highway_mpg']).reshape(-1, 1),y_train)\n",
    "y_pred = model_one_feature.predict(np.array(df_val['highway_mpg']).reshape(-1, 1))\n",
    "highway_mpg_accuracy = round(accuracy_score(y_val, y_pred),3)\n",
    "print(f\"Accuracy on the validation dataset with just highway_mpg: {highway_mpg_accuracy}\")\n",
    "\n",
    "highway_mpg_auc = round(roc_auc_score(y_val, y_pred),3)\n",
    "print(f\"AUC on the validation dataset with just highway_mpg: {highway_mpg_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_feature = LogisticRegression(solver='liblinear', C=1, max_iter=1000, random_state=1)\n",
    "model_one_feature.fit(np.array(df_train['city_mpg']).reshape(-1, 1),y_train)\n",
    "y_pred = model_one_feature.predict(np.array(df_val['city_mpg']).reshape(-1, 1))\n",
    "city_mpg_accuracy = round(accuracy_score(y_val, y_pred),3)\n",
    "print(f\"Accuracy on the validation dataset with just city_mpg: {city_mpg_accuracy}\")\n",
    "\n",
    "city_mpg_auc = round(roc_auc_score(y_val, y_pred),3)\n",
    "print(f\"AUC on the validation dataset with just highway_mpg: {city_mpg_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Training the model\n",
    "Apply one-hot-encoding using DictVectorizer and train the logistic regression with these parameters:\n",
    "\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "0.678\n",
    "0.779\n",
    "0.878\n",
    "0.979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = categorical + numerical\n",
    "\n",
    "train_dicts = df_train[columns].to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_dicts = df_val[columns].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "     \n",
    "auc_val = round(roc_auc_score(y_val, y_pred),3)     \n",
    "print(f\"AUC on the validation dataset: {auc_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "print(f\"AUC on the validation dataset: {auc_val}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(fpr, tpr, color='black')\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=0.7, linestyle='dashed', alpha=0.5)\n",
    "\n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('ROC curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> AUC on the validation dataset is 0.979 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Precision and Recall\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "For each threshold, compute precision and recall\n",
    "Plot them\n",
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "0.28\n",
    "0.48\n",
    "0.68\n",
    "0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "scores = []\n",
    "\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "for t in thresholds: #B\n",
    "    tp = ((y_pred >= t) & (y_val == 1)).sum()\n",
    "    fp = ((y_pred >= t) & (y_val == 0)).sum()\n",
    "    fn = ((y_pred < t) & (y_val == 1)).sum()\n",
    "    tn = ((y_pred < t) & (y_val == 0)).sum()\n",
    "    scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores.columns = ['threshold', 'tp', 'fp', 'fn', 'tn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(df_scores.threshold, df_scores.tpr, color='black', linestyle='solid', label='TPR')\n",
    "plt.plot(df_scores.threshold, df_scores.fpr, color='black', linestyle='dashed', label='FPR')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "plt.yticks(np.linspace(0, 1, 11))\n",
    "\n",
    "plt.xlabel('Thresholds')\n",
    "plt.title('TPR and FPR')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['precision'] = df_scores.tp/(df_scores.tp + df_scores.fp)\n",
    "df_scores['recall'] = df_scores.tp/(df_scores.tp + df_scores.fn)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(df_scores.threshold, df_scores.precision, color='black', linestyle='solid', label='Precision')\n",
    "plt.plot(df_scores.threshold, df_scores.recall, color='black', linestyle='dashed', label='Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "plt.yticks(np.linspace(0, 1, 11))\n",
    "\n",
    "plt.xlabel('Thresholds')\n",
    "plt.title('Precision and Recall')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.iloc[47:51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The precision and recall curves intersect at threshold = 0.48 (nearest answer) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: F1 score\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    " \n",
    "\n",
    "Where \n",
    " is precision and \n",
    " is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "At which threshold F1 is maximal?\n",
    "\n",
    "0.12\n",
    "0.32\n",
    "0.52\n",
    "0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['F1'] = (2* df_scores.precision * df_scores.recall)/(df_scores.precision + df_scores.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.iloc[np.linspace(12, 72, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_scores.F1)\n",
    "\n",
    "max_F1_idx = df_scores[\"F1\"].idxmax()\n",
    "optimal_threshold = df_scores.loc[max_F1_idx, \"threshold\"]\n",
    "\n",
    "print(f\"The maximal F1 score is {df_scores['F1'][max_F1_idx]:.2f} at threshold {optimal_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.iloc[40:59]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> F1 is maximal at threshold = 0.52 (nearest answer) </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: 5-Fold CV\n",
    "--------------------------\n",
    "Use the KFold class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "Iterate over different folds of df_full_train\n",
    "Split the data into train and validation\n",
    "Train the model on train with these parameters: LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "Use AUC to evaluate the model on validation\n",
    "How large is standard devidation of the scores across different folds?\n",
    "\n",
    "0.003\n",
    "0.030\n",
    "0.090\n",
    "0.140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, y):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    dv.fit(cat)\n",
    "\n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return dv, model\n",
    "\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "aucs = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_train_full):\n",
    "    df_train = df_train_full.iloc[train_idx]\n",
    "    y_train = df_train.above_average\n",
    "\n",
    "    df_val = df_train_full.iloc[val_idx]\n",
    "    y_val = df_val.above_average\n",
    "\n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    rocauc = roc_auc_score(y_val, y_pred)\n",
    "    aucs.append(rocauc)\n",
    "    \n",
    "np.array(aucs).round(3)\n",
    "\n",
    "print('auc = %0.3f ± %0.3f' % (np.mean(aucs), np.std(aucs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The standard devidation of the scores across different folds is 0.003 (nearest answer) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Hyperparemeter Tuning\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "\n",
    "Iterate over the following C values: [0.01, 0.1, 0.5, 10]\n",
    "Initialize KFold with the same parameters as previously\n",
    "Use these parametes for the model: LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "Which C leads to the best mean score?\n",
    "\n",
    "0.01\n",
    "0.1\n",
    "0.5\n",
    "10\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, y, C=1.0):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    dv.fit(cat)\n",
    "\n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "nfolds = 5\n",
    "kfold = KFold(n_splits=nfolds, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "for C in [0.01,0.1,0.5,10]:\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_train_full):\n",
    "        df_train = df_train_full.iloc[train_idx]\n",
    "        y_train = df_train.above_average\n",
    "        \n",
    "        df_val = df_train_full.iloc[val_idx]\n",
    "        y_val = df_val.above_average\n",
    "        \n",
    "        dv, model = train(df_train, y_train, C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "        \n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    print('C=%s, auc = %0.3f ± %0.3f' % (C, np.mean(aucs), np.std(aucs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> C = 10 gives the highest AUC </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
